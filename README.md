# ML_Basics
Learning and experimenting with basic concepts and tools used in Machine Learning

Content:

## Regression

1. Multiple_Linear_Regression.ipynb
  * Implement Multiple Linear Regression
2. Vectorization.ipynb
  * Demonstrate common uses of vectors and matrices in Python, as typically applied in machine learning implementations.
3. Model_Representation.ipynb
  * Implement the model  ùëìùë§,ùëè for linear regression with one variable.
  * Experiment with different w and b.
5. Feature scaling and learning rate.ipynb
  * Run Gradient Descent on a data set with multiple features.
  * Explore the impact of the learning rate alpha on gradient descent.
  * Improve performance of gradient descent by feature scaling using z-score normalization.

## Classification

1. Classification_Basics.ipynb
  * Contrast regression and classification.
  * Explore the sigmoid function (also known as the logistic function).
  * Explore logistic regression; which uses the sigmoid function.
  * Plot the decision boundary for a logistic regression model.
2. Logistic Regression-Logistic Loss.ipynb
  * Explore the reason the squared error loss is not appropriate for logistic regression.
  * Explore the logistic loss function.
3. Cost Function for Logistic Regression.ipynb
  * Examine the implementation and utilize the cost function for logistic regression.
4. Gradient Descent for Logistic Regression.ipynb
  * Update gradient descent for logistic regression.
  * Explore gradient descent on a familiar data set.
5. Logistic Regression using Scikit-Learn.ipynb
  * Train a logistic regression model using scikit-learn.

## Common
1. Overfitting.ipynb
  * Demonstrate the situations where overfitting can occur some of the solutions.
2. Regularized Cost and Gradient
  * Extend the previous linear and logistic cost functions with a regularization term.
  * Rerun the previous example of over-fitting with a regularization term added.
    
